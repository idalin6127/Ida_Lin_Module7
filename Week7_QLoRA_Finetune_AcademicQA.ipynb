{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3f56fd",
   "metadata": {},
   "source": [
    "# Week 7: Synthetic Data Generation & QLoRA Fine-Tuning (Academic Q&A)\n",
    "**Pipeline:** abstracts → GPT-4 Q&A → JSONL → QLoRA (Unsloth) → Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bed290",
   "metadata": {},
   "source": [
    "## 0. Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on Colab, use the provided requirements file:\n",
    "# from google.colab import files\n",
    "# files.upload()  # upload requirements.txt if needed\n",
    "# !pip install -r requirements.txt\n",
    "# Or install manually:\n",
    "# !pip install unsloth transformers peft bitsandbytes datasets accelerate pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf2000",
   "metadata": {},
   "source": [
    "## 1. Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful academic Q&A assistant specialized in scholarly content.\"\n",
    "DATA_DIR = \"./data\"  # change if needed\n",
    "JSONL_PATH = f\"{DATA_DIR}/synthetic_qa.jsonl\"\n",
    "MODEL_NAME = \"unsloth/llama-3.1-7b-unsloth-bnb-4bit\"\n",
    "OUTPUT_DIR = \"./llama3-7b-qlora-finetuned\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d95f5",
   "metadata": {},
   "source": [
    "## 2. (Optional) Generate Q&A with GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder structure for aggregated Q&A list:\n",
    "# papers_qas = [\n",
    "#   {\n",
    "#     \"paper_id\": \"arxiv:2401.12345\",\n",
    "#     \"title\": \"Title...\",\n",
    "#     \"qas\": [\n",
    "#       {\"question\": \"What is the main contribution?\", \"answer\": \"The paper proposes ...\"},\n",
    "#       # total ~5 per paper\n",
    "#     ]\n",
    "#   },\n",
    "#   # ... ~100 papers\n",
    "# ]\n",
    "# with open(f\"{DATA_DIR}/papers_qas.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(papers_qas, f, ensure_ascii=False, indent=2)\n",
    "print(\"Skip if you already created Q&A JSON.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa95016",
   "metadata": {},
   "source": [
    "## 3. Convert Q&A to instruction-tuning JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def to_jsonl_from_aggregated(input_path, jsonl_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "    n = 0\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as w:\n",
    "        for it in items:\n",
    "            for qa in it.get(\"qas\", []):\n",
    "                q = qa[\"question\"].strip()\n",
    "                a = qa[\"answer\"].strip()\n",
    "                full_prompt = f\"<|system|>{SYSTEM_PROMPT}<|user|>{q}<|assistant|>{a}\"\n",
    "                w.write(json.dumps({\"text\": full_prompt}, ensure_ascii=False) + \"\\n\")\n",
    "                n += 1\n",
    "    print(f\"JSONL written: {jsonl_path} with {n} pairs\")\n",
    "\n",
    "# Example (uncomment when you have papers_qas.json):\n",
    "# to_jsonl_from_aggregated(f\"{DATA_DIR}/papers_qas.json\", JSONL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0399c0f",
   "metadata": {},
   "source": [
    "## 4. QLoRA Fine-Tuning (Unsloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, SFTTrainer\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = FastLanguageModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=JSONL_PATH, split=\"train\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\"\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved to\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b74a45",
   "metadata": {},
   "source": [
    "## 5. Evaluation (Base vs Fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c85657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TEST_QUESTIONS = [\n",
    "    \"Summarize the main hypothesis of the paper on [topic].\",\n",
    "    \"How did the authors evaluate their model? What metrics were used?\",\n",
    "    \"What are the key limitations discussed by the authors?\",\n",
    "    \"Which baselines were compared and how did the proposed method perform?\",\n",
    "    \"Describe the core algorithmic novelty introduced in the study.\",\n",
    "    \"What datasets were used and why are they appropriate?\",\n",
    "    \"How does the method generalize across tasks or domains?\",\n",
    "    \"What ablation studies were conducted and what did they show?\",\n",
    "    \"What future work do the authors suggest?\",\n",
    "    \"Explain the difference between this method and a closely related approach.\"\n",
    "]\n",
    "\n",
    "def answer_question(model, tokenizer, q):\n",
    "    prompt = f\"<|system|>{SYSTEM_PROMPT}<|user|>{q}<|assistant|>\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # If running on GPU: inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "    text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "    if \"<|assistant|>\" in text:\n",
    "        text = text.split(\"<|assistant|>\", 1)[-1].strip()\n",
    "    return text\n",
    "\n",
    "# Load base and ft models\n",
    "base_model = FastLanguageModel.from_pretrained(MODEL_NAME)\n",
    "ft_model   = FastLanguageModel.from_pretrained(OUTPUT_DIR)\n",
    "tok        = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "rows = []\n",
    "for i, q in enumerate(TEST_QUESTIONS, 1):\n",
    "    base_ans = answer_question(base_model, tok, q)\n",
    "    ft_ans   = answer_question(ft_model, tok, q)\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    print(\"Base:\", base_ans[:800])\n",
    "    print(\"FT  :\", ft_ans[:800])\n",
    "    print(\"-\"*80)\n",
    "    rows.append({\"id\": i, \"question\": q, \"base_answer\": base_ans, \"ft_answer\": ft_ans})\n",
    "\n",
    "# Optionally write to CSV for manual scoring\n",
    "import pandas as pd\n",
    "pd.DataFrame(rows).to_csv(\"eval_raw_outputs.csv\", index=False)\n",
    "print(\"Saved eval_raw_outputs.csv\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
